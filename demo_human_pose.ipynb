{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"mylogs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco_humanpose.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "#COCO_DIR = \"D:/Github/FastMaskRCNN/data/coco\"  # TODO: enter value here\n",
    "#IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "COCO_DIR = \"E:/DSLR/01_Privat/2020/Hochzeit/Daniel/Fotos_Reportage\"  # TODO: enter value here\n",
    "IMAGE_DIR = COCO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "(1, 1000, 4)\n",
      "(None, None, 2)\n",
      "(None, None, 2, 4)\n",
      "(None, None)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\model.py:1074 call  *\n        outputs = utils.batch_slice(\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\utils.py:994 batch_slice  *\n        output_slice = graph_fn(*inputs_slice)\n    C:\\Users\\LStue\\AppData\\Local\\Temp\\tmpw8yt_v14.py:15 None  *\n        outputs = ag__.converted_call(utils.batch_slice, ([rois, mrcnn_class, mrcnn_bbox, window], lambda x, y, w, z: ag__.converted_call(refine_detections_graph, (x, y, w, z, self.config), None, fscope), self.config.IMAGES_PER_GPU), None, fscope)\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\model.py:959 refine_detections_graph  *\n        indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:1430 range\n        limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1314 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:317 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:258 constant\n        allow_broadcast=True)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:296 _constant_impl\n        allow_broadcast=allow_broadcast))\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py:439 make_tensor_proto\n        raise ValueError(\"None values not supported.\")\n\n    ValueError: None values not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-575fd09b2cc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m model = modellib.MaskRCNN(mode=\"inference\", \n\u001b[0;32m     10\u001b[0m                           \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                           model_dir=MODEL_DIR)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Get path to saved weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Mask_RCNN_Humanpose\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2530\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2531\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Mask_RCNN_Humanpose\\model.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, mode, config)\u001b[0m\n\u001b[0;32m   2775\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image_meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m             detections = DetectionLayer(config, name=\"mrcnn_detection\")(\n\u001b[1;32m-> 2777\u001b[1;33m                 [rpn_rois, mrcnn_class, mrcnn_bbox,input_image_meta])\n\u001b[0m\u001b[0;32m   2778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m             \u001b[1;31m# Convert boxes to normalized coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\model.py:1074 call  *\n        outputs = utils.batch_slice(\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\utils.py:994 batch_slice  *\n        output_slice = graph_fn(*inputs_slice)\n    C:\\Users\\LStue\\AppData\\Local\\Temp\\tmpw8yt_v14.py:15 None  *\n        outputs = ag__.converted_call(utils.batch_slice, ([rois, mrcnn_class, mrcnn_bbox, window], lambda x, y, w, z: ag__.converted_call(refine_detections_graph, (x, y, w, z, self.config), None, fscope), self.config.IMAGES_PER_GPU), None, fscope)\n    C:\\Users\\LStue\\Mask_RCNN_Humanpose\\model.py:959 refine_detections_graph  *\n        indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:1430 range\n        limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1314 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:317 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:258 constant\n        allow_broadcast=True)\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py:296 _constant_impl\n        allow_broadcast=allow_broadcast))\n    D:\\Users\\LStue\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py:439 make_tensor_proto\n        raise ValueError(\"None values not supported.\")\n\n    ValueError: None values not supported.\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    KEYPOINT_MASK_POOL_SIZE = 7\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "\n",
    "model_path = os.path.join(ROOT_DIR, \"mask_rcnn_coco_humanpose.h5\")\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# COCO Class names\n",
    "#For human pose task We just use \"BG\" and \"person\"\n",
    "class_names = ['BG', 'person']\n",
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "# image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "image = cv2.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "#BGR->RGB\n",
    "image = image[:,:,::-1]\n",
    "\n",
    "# Run detection\n",
    "results = model.detect_keypoint([image], verbose=1)\n",
    "r = results[0] # for one image\n",
    "\n",
    "log(\"rois\",r['rois'])\n",
    "log(\"keypoints\",r['keypoints'])\n",
    "log(\"class_ids\",r['class_ids'])\n",
    "log(\"keypoints\",r['keypoints'])\n",
    "log(\"masks\",r['masks'])\n",
    "log(\"scores\",r['scores'])\n",
    "\n",
    "visualize.display_keypoints(image,r['rois'],r['keypoints'],r['class_ids'],class_names,skeleton = inference_config.LIMBS)\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
